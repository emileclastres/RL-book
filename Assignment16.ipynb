{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.5 64-bit (conda)",
   "metadata": {
    "interpreter": {
     "hash": "38769b1f53c72ff86450cf729f4dd42eb6dc200bf64e42232d27ac83b9528a7d"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Assignment 16: Policy Gradient\n",
    "\n",
    "## 1) REINFORCE using pytorch interface\n",
    "\n",
    "Let's build a class that will be at thye same time a pytorch DNN (with softmax output layer) and a Policy. REINFORCE will then simply compute traces and update the weights.\n",
    "\n",
    "For the weight update, we can use the SGD optimizer, and perform backward passes on $-\\alpha \\gamma^t G_t \\log(\\pi(s,a))$ to update the DNN's weights according to the course's pseudo code.\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from rl.distribution import Choose, Distribution\n",
    "from rl.markov_decision_process import MarkovDecisionProcess, Policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleSigmoidModel(torch.nn.Module):\n",
    "    def __init__(self, n_layers, input_size, num_actions):\n",
    "        super().__init__()\n",
    "        def build_base_bloc():\n",
    "            return torch.nn.Sequential(torch.nn.Linear(input_size,input_size))\n",
    "        list_modules =  [torch.nn.Sequential(build_base_bloc(), torch.nn.Sigmoid()) for i in range(n_layers-1)]  + [torch.nn.Sequential(torch.nn.Linear(input_size,num_actions), torch.nn.Softmax(dim = 1))]\n",
    "        self.model = torch.nn.Sequential(*list_modules)\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "\n",
    "\n",
    "class torchPolicyDiscrete(torch.nn.Module, Policy):\n",
    "    def __init__(self, n_layers, feature_extractors, action_space, learning_rate):\n",
    "        super().__init__()\n",
    "        self.model = SimpleSigmoidModel(n_layers, len(feature_extractors), len(action_space))\n",
    "        self.feature_extractors = feature_extractors\n",
    "        self.action_space = action_space\n",
    "        self.action_indexes = {a:i for i,a in enumerate(self.action_space)}\n",
    "        self.optimizer = torch.optim.SGD(self.model.parameters(), lr=learning_rate)\n",
    "\n",
    "    def forward(self, s_tensor):\n",
    "        return self.model(s_tensor)\n",
    "\n",
    "    def extract_features(self, s):\n",
    "        return torch.tensor([[phi_i(s) for phi_i in self.feature_extractors]])\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def distribution_action(self, s):\n",
    "        probas = self.model(s).numpy().flatten()\n",
    "        return Choose({a:probas[i] for i,a in enumerate(self.action_space)})\n",
    "    def act(self, s):\n",
    "        return self.distribution_action(s).sample()\n",
    "\n",
    "    def update_params(self, s,a,G,gamma,t):\n",
    "        a_index = self.action_indexes[a]\n",
    "        s_tensor = self.extract_features(s)\n",
    "        pi_s_a = self.forward(s_tensor)[0, a_index]\n",
    "        loss = -(gamma**t)*G*torch.log(pi_s_a)\n",
    "\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "\n",
    "def sample_one_episode_SAG(policy : torchPolicyDiscrete, mdp: MarkovDecisionProcess, init_state_distrib : Distribution, gamma : float):\n",
    "    sasr_seq = [sasr for sasr in mdp.simulate_actions( start_states = init_state_distrib, policy = policy)]\n",
    "    r_seq = [r for (s,a,snext,r) in sasr_seq]\n",
    "    G_seq = []\n",
    "    for r in r_seq[::-1]:\n",
    "        if len(G_seq) == 0:\n",
    "            G_seq.append(r)\n",
    "        else:\n",
    "            G_seq.append(r + G_seq[-1]*gamma)\n",
    "    G_seq = G_seq[::-1]\n",
    "    sag_seq = [(s,a,G) for ((s,a,snext,r), G) in zip(sasr_seq, G_seq)]\n",
    "    return sag_seq\n",
    "\n",
    "def update_policy_with_episode(policy : torchPolicyDiscrete, sag_seq, gamma : float):\n",
    "    for t, (s,a,G) in enumerate(sag_seq):\n",
    "        policy.update_params(s,a,G,gamma, t)\n",
    "\n",
    "\n",
    "def REINFORCE(policy : torchPolicyDiscrete,gamma : float, mdp : MarkovDecisionProcess, init_state_distrib : Distribution, n_episodes : int):\n",
    "    for _ in range(n_episodes):\n",
    "        sag_seq = sample_one_episode_SAG(policy, mdp, init_state_distrib, gamma)\n",
    "        update_policy_with_episode(policy, sag_seq, gamma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "source": [
    "## 3) Some math\n",
    "\n",
    "$$\n",
    "\\pi(s, a ; \\boldsymbol{\\theta})=\\frac{e^{\\phi(s, a)^{T} \\cdot \\boldsymbol{\\theta}}}{\\sum_{b \\in \\mathcal{A}} e^{\\phi(s, b)^{T} \\cdot \\boldsymbol{\\theta}}}\n",
    "$$\n",
    "\n",
    "Therefore \n",
    "\n",
    "$$\n",
    "\\log(\\pi(s, a ; \\boldsymbol{\\theta}))=\\phi(s, a)^{T} \\cdot \\boldsymbol{\\theta} - \\log(\\sum_{b \\in \\mathcal{A}} e^{\\phi(s, b)^{T} \\cdot \\boldsymbol{\\theta}})\n",
    "$$\n",
    "\n",
    "Taking the derivative:\n",
    "\n",
    "\n",
    "$$\n",
    "\\nabla_\\theta \\log(\\pi(s, a ; \\boldsymbol{\\theta}))=\\phi(s, a)  -  \\nabla_\\theta\\log(\\sum_{b \\in \\mathcal{A}} e^{\\phi(s, b)^{T} \\cdot \\boldsymbol{\\theta}}) = \\phi(s, a)  - \\frac{ \\nabla_\\theta \\sum_{b \\in \\mathcal{A}} e^{\\phi(s, b)^{T} \\cdot \\boldsymbol{\\theta}}}{\\sum_{b \\in \\mathcal{A}} e^{\\phi(s, b)^{T} \\cdot \\boldsymbol{\\theta}}}\n",
    "$$\n",
    "\n",
    "Which gives \n",
    "$$\n",
    "\\nabla_\\theta \\log(\\pi(s, a ; \\boldsymbol{\\theta})) = \\phi(s, a)  - \\frac{ \\sum_{b \\in \\mathcal{A}}\\phi(s, b) e^{\\phi(s, b)^{T} \\cdot \\boldsymbol{\\theta}}}{\\sum_{b \\in \\mathcal{A}} e^{\\phi(s, b)^{T} \\cdot \\boldsymbol{\\theta}}} = \\phi(s, a)  -  \\sum_{b \\in \\mathcal{A}} \\phi(s, b) \\pi(s, a ; \\boldsymbol{\\theta})\n",
    "$$\n",
    "\n",
    "Therefore : \n",
    "$$\n",
    "\\nabla_\\theta \\log(\\pi(s, a ; \\boldsymbol{\\theta})) =  \\phi(s, a)  -  \\mathrm{E}_{\\pi}[\\phi(s, \\cdot)]\n",
    "$$\n",
    "\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}