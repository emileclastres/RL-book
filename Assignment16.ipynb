{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.5 64-bit (conda)",
   "metadata": {
    "interpreter": {
     "hash": "38769b1f53c72ff86450cf729f4dd42eb6dc200bf64e42232d27ac83b9528a7d"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Assignment 16: Policy Gradient\n",
    "\n",
    "## 1) REINFORCE using pytorch interface\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from rl.distribution import Choose, Distribution\n",
    "from rl.markov_decision_process import MarkovDecisionProcess, Policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-2-69259faae8b8>, line 5)",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-2-69259faae8b8>\"\u001b[1;36m, line \u001b[1;32m5\u001b[0m\n\u001b[1;33m    list_modules =  [torch.nn.Sequential(build_base_bloc(), torch.nn.Sigmoid()) for i in range n_layers[:-1]] + [torch.nn.Sequential(torch.nn.Linear(input_size,num_actions), torch.nn.Softmax(num_actions))]\u001b[0m\n\u001b[1;37m                                                                                               ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "class SimpleSigmoidModel(torch.nn.Module):\n",
    "    def __init__(self, n_layers, input_size, num_actions):\n",
    "        super().__init__()\n",
    "        def build_base_bloc():\n",
    "            return torch.nn.Sequential(torch.nn.Linear(input_size,input_size))\n",
    "        list_modules =  [torch.nn.Sequential(build_base_bloc(), torch.nn.Sigmoid()) for i in range n_layers[:-1]] + [torch.nn.Sequential(torch.nn.Linear(input_size,num_actions), torch.nn.Softmax(num_actions))]\n",
    "        self.model = torch.nn.Sequential(*list_modules)\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "\n",
    "\n",
    "class torchPolicyDiscrete(torch.nn.Module, Policy):\n",
    "    def __init__(self, n_layers, feature_extractors, action_space, learning_rate):\n",
    "        super().__init__()\n",
    "        self.model = SimpleSigmoidModel(n_layers, len(feature_extractors), len(action_space))\n",
    "        self.feature_extractors = feature_extractors\n",
    "        self.action_space = action_space\n",
    "        self.action_indexes = {a:i for i,a in enumerate(self.action_space)}\n",
    "        self.optimizer = torch.optim.SGD(self.model.parameters(), lr=learning_rate)\n",
    "\n",
    "    def forward(self, s):\n",
    "        return self.model(self.extract_features(s))\n",
    "\n",
    "    def extract_features(self, s):\n",
    "        return torch.tensor([phi_i(s) for phi_i in self.feature_extractors]).view(1,-1)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def distribution_action(self, s):\n",
    "        probas = self.model(s).numpy().flatten()\n",
    "        return Choose({a:probas[i] for i,a in enumerate(self.action_space)})\n",
    "    def act(self, s):\n",
    "        return self.distribution_action(s).sample()\n",
    "\n",
    "    def update_params(self, s,a,G,gamma,t):\n",
    "        a_index = self.action_indexes[a]\n",
    "        pi_s_a = self.forward(s)[0, a_index]\n",
    "        loss = -(gamma**t)*G*torch.log(pi_s_a)j\n",
    "\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "\n",
    "def sample_one_episode(policy : torchPolicyDiscrete, mdp: MarkovDecisionProcess, init_state_distrib : Distribution):\n",
    "    return mdp.simulate_actions( start_states = init_state_distrib, policy = policy)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}