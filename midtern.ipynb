{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CME 241: MIDTERM\n",
    "#### Emile Clastres - clastres@stanford.edu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 1: Value Iteration Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a)\n",
    "The question is unclear abot whether our formulation should be general or relative to the provided maze. We are going to solve the provided maze, but let us state that it can be generalized by considering that the goal is located at position $(x_*, y_*)$ and that the grid size is from $0$ to $n$ instead of $0$ to $7$. In what follows, every variable $x, y$ is an integer.\n",
    "\n",
    "The state-space is $\\mathcal{S} = \\{(x,y) | 0 \\leq x,y \\leq 7\\}$, with $\\mathcal{T} = \\{(7,7)\\}$\n",
    "The unrestricted action space is $\\mathcal{A} = \\{u, d, l, r\\}$ denoting UP, DOWN, LEFT and RIGHT. We will precise them further below.\n",
    "\n",
    "Let us introduce two functions that are going to facilitate notations:\n",
    "\n",
    "the function $space(s)$ is true if and only if the position $(x,y)$ is a space (or the goal), for $s \\in \\mathcal{S}$.\n",
    "the function $move((x,y),a)$ returns $(x-1, y)$ if $a = u$, $(x+1, y)$ if $a = d$, $(x, y+1)$ if $a = r$, $(x, y-1)$ if $a = l$\n",
    "\n",
    "We can now precise the restricted action space, filtering for illegal moves.\n",
    "\n",
    "$\\forall s\\in \\mathcal{S}, \\mathcal{A}(s) = \\{a | ~a \\in \\mathcal{A}:~ space(move(s,a)) ~\\&~ move(s,a) \\in \\mathcal{S} \\}$\n",
    "\n",
    "With these notations, $\\forall s \\in \\mathcal{S},~ \\forall a \\in \\mathcal{A}(s), ~ \\mathcal{P}(s,a,move(s,a)) = 1$\n",
    "\n",
    "Note that this specification is not yet rigorous, as $\\mathcal{P}(s, \\cdot, \\cdot)$ is not defined for $s$ that are blocks. We think that the above specification is however the clearest. \n",
    "\n",
    "The rigorous specification is:\n",
    "\n",
    "$\\mathcal{S} = \\{(x,y) | 0 \\leq x,y \\leq 7 ~ \\& ~ space(x,y)\\}$\n",
    "\n",
    "$\\forall s\\in \\mathcal{S}, \\mathcal{A}(s) = \\{a | ~a \\in \\mathcal{A}: move(s,a) \\in \\mathcal{S} \\}$\n",
    "$\\forall s \\in \\mathcal{S},~ \\forall a \\in \\mathcal{A}(s), ~ \\mathcal{P}(s,a,move(s,a)) = 1$\n",
    "\n",
    "This specification is complete, and for exhaustivity we must state here that this definition assumes that it is not possible for a position $(x,y)$ to be surrounded by blocks unless it is the only position in S.\n",
    "\n",
    "\n",
    "### Reward version 1:\n",
    "\n",
    "$\\gamma = 1$, $\\forall s \\in \\mathcal{S},~ \\forall a \\in \\mathcal{A}(s), ~ \\mathcal{R}_T(s,a) = 1 ~~\\text{if} ~~ move(s,a) \\in \\mathcal{T}, ~\\text{else} ~ \\mathcal{R}_T(s,a) = 0$\n",
    "\n",
    "### Reward version 2:\n",
    "\n",
    "$\\gamma < 1$, $\\forall s \\in \\mathcal{S},~ \\forall a \\in \\mathcal{A}(s), ~ \\mathcal{R}(s,a) = -1 $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Tuple, Mapping, Sequence, Optional\n",
    "from rl.markov_decision_process import FiniteMarkovDecisionProcess, FinitePolicy, StateActionMapping\n",
    "from rl.distribution import Constant\n",
    "\n",
    "\n",
    "SPACE = 'SPACE'\n",
    "BLOCK = 'BLOCK'\n",
    "GOAL = 'GOAL'\n",
    "\n",
    "maze_grid = {(0, 0): SPACE, (0, 1): BLOCK, (0, 2): SPACE, (0, 3): SPACE, (0, 4): SPACE, \n",
    "             (0, 5): SPACE, (0, 6): SPACE, (0, 7): SPACE, (1, 0): SPACE, (1, 1): BLOCK,\n",
    "             (1, 2): BLOCK, (1, 3): SPACE, (1, 4): BLOCK, (1, 5): BLOCK, (1, 6): BLOCK, \n",
    "             (1, 7): BLOCK, (2, 0): SPACE, (2, 1): BLOCK, (2, 2): SPACE, (2, 3): SPACE, \n",
    "             (2, 4): SPACE, (2, 5): SPACE, (2, 6): BLOCK, (2, 7): SPACE, (3, 0): SPACE, \n",
    "             (3, 1): SPACE, (3, 2): SPACE, (3, 3): BLOCK, (3, 4): BLOCK, (3, 5): SPACE, \n",
    "             (3, 6): BLOCK, (3, 7): SPACE, (4, 0): SPACE, (4, 1): BLOCK, (4, 2): SPACE, \n",
    "             (4, 3): BLOCK, (4, 4): SPACE, (4, 5): SPACE, (4, 6): SPACE, (4, 7): SPACE, \n",
    "             (5, 0): BLOCK, (5, 1): BLOCK, (5, 2): SPACE, (5, 3): BLOCK, (5, 4): SPACE, \n",
    "             (5, 5): BLOCK, (5, 6): SPACE, (5, 7): BLOCK, (6, 0): SPACE, (6, 1): BLOCK, \n",
    "             (6, 2): BLOCK, (6, 3): BLOCK, (6, 4): SPACE, (6, 5): BLOCK, (6, 6): SPACE, \n",
    "             (6, 7): SPACE, (7, 0): SPACE, (7, 1): SPACE, (7, 2): SPACE, (7, 3): SPACE, \n",
    "             (7, 4): SPACE, (7, 5): BLOCK, (7, 6): BLOCK, (7, 7): GOAL}\n",
    "\n",
    "UP = 'UP'\n",
    "DOWN = 'DOWN'\n",
    "LEFT = 'LEFT'\n",
    "RIGHT = 'RIGHT'\n",
    "\n",
    "ACTIONS = [UP, DOWN, LEFT, RIGHT]\n",
    "\n",
    "Action = str\n",
    "Position = Tuple[int, int]\n",
    "\n",
    "class MazeMDP(FiniteMarkovDecisionProcess):\n",
    "    gamma : float\n",
    "    \n",
    "    def __init__(self, maze_grid : Mapping[Position, Action] = maze_grid,\n",
    "                         gamma : float = 1.):        \n",
    "        self.gamma = gamma\n",
    "        n : int = max(x for x,_ in maze_grid)\n",
    "        m : int = max(y for _,y in maze_grid)\n",
    "        \n",
    "        def reward(s) -> float:\n",
    "            \"\"\"reward gained from arriving at state s\"\"\"\n",
    "            if gamma >= 1.:\n",
    "                return -1.\n",
    "            else:\n",
    "                return float(maze_grid[s] is GOAL)\n",
    "            \n",
    "        def available_actions(s) -> Sequence[Action]:\n",
    "            return [a for a in ACTIONS if (move(s,a) in maze_grid) and \\\n",
    "                                (maze_grid[move(s,a)] in [SPACE, GOAL])]\n",
    "    \n",
    "        def move(s : Position, a : Action) -> Position:\n",
    "            \"\"\"Deterministic movement from state s with direction a\"\"\"\n",
    "            x,y = s\n",
    "            assert a in ACTIONS\n",
    "            if a == UP:\n",
    "                return (x-1, y)\n",
    "            elif a == DOWN:\n",
    "                return (x+1, y)\n",
    "            elif a == LEFT:\n",
    "                return (x, y-1)\n",
    "            else:\n",
    "                return (x, y+1)\n",
    "        \n",
    "        sa_mapping : StateActionMapping[Position, Action, Position] = {}\n",
    "        for x in range(n + 1):\n",
    "            for y in range(m + 1):\n",
    "                s = (x,y)\n",
    "                if s in maze_grid and maze_grid[s] is SPACE:\n",
    "                    sa_mapping[s] = {}\n",
    "                    for a in available_actions(s):\n",
    "                        s_ = move(s, a)\n",
    "                        sa_mapping[s][a] = Constant((s_, reward(s_)))\n",
    "                    if len(available_actions(s)) == 0:\n",
    "                        sa_mapping[s][a] = None\n",
    "                elif s is GOAL:\n",
    "                    sa_mapping[s] = None\n",
    "\n",
    "        super().__init__(sa_mapping)\n",
    "        \n",
    "    \n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rl.dynamic_programming import value_iteration_result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We modified the source code for value_iteration_result in order to take an optionnal argument print_steps which, if True, prints the number of iterations.\n",
    "The source code is available on my repo as well as in my Assignment 4.\n",
    "\n",
    "It is copied below, but needs to be present in rl.dynamic_programming and rl.iteration to be run\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def converged_counted(values: Iterator[X],\n",
    "                    done: Callable[[X, X], bool]) -> X:\n",
    "    '''Return the final value of the given iterator and prints \n",
    "    the number of iterations when its values\n",
    "    converge according to the done function.\n",
    "\n",
    "    Raises an error if the iterator is empty.\n",
    "\n",
    "    Will loop forever if the input iterator doesn't end *or* converge.\n",
    "    '''\n",
    "    result = None\n",
    "    count:int = 0\n",
    "    for val in converge(values, done):\n",
    "        count+=1\n",
    "        result = val\n",
    "\n",
    "    if result is None:\n",
    "        raise ValueError(\"converged called on an empty iterator\")\n",
    "    else:\n",
    "        print(f\"Converged in {n} iterations.\")\n",
    "    return result\n",
    "\n",
    "\n",
    "def policy_iteration_result(\n",
    "    mdp: FiniteMarkovDecisionProcess[S, A],\n",
    "    gamma: float,\n",
    "    print_steps: bool = False\n",
    ") -> Tuple[V[S], FinitePolicy[S, A]]:\n",
    "\tif not print_steps:\n",
    "    \treturn converged(policy_iteration(mdp, gamma), done=almost_equal_vf_pis)\n",
    "    else:\n",
    "    \treturn converged_counted(policy_iteration(mdp, gamma), done=almost_equal_vf_pis)\n",
    "\n",
    "\n",
    "def value_iteration_result(\n",
    "    mdp: FiniteMarkovDecisionProcess[S, A],\n",
    "    gamma: float,\n",
    "    print_steps: bool = False\n",
    ") -> Tuple[V[S], FinitePolicy[S, A]]:\n",
    "\tif not print_steps:\n",
    "\t    opt_vf: V[S] = converged(\n",
    "\t        value_iteration(mdp, gamma),\n",
    "\t        done=almost_equal_vfs\n",
    "\t    )\n",
    "\telse:\n",
    "\t\topt_vf: V[S] = converged_counted(\n",
    "\t        value_iteration(mdp, gamma),\n",
    "\t        done=almost_equal_vfs\n",
    "\t    )\n",
    "    opt_policy: FinitePolicy[S, A] = greedy_policy_from_vf(\n",
    "        mdp,\n",
    "        opt_vf,\n",
    "        gamma\n",
    "    )\n",
    "\n",
    "    return opt_vf, opt_policy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's take a look at the convergence speed depending on gamma:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converged in 17 iterations.\n"
     ]
    }
   ],
   "source": [
    "maze_v1 = MazeMDP(maze_grid = maze_grid, gamma  = 1.)\n",
    "opt_vf_v1, opt_policy_v1 = value_iteration_result(mdp = maze_v1, gamma = maze_v1.gamma, print_steps = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{(0, 0): -16.0, (0, 2): -12.0, (0, 3): -11.0, (0, 4): -12.0,\n"
     ]
    }
   ],
   "source": [
    "print(opt_vf_v1.__repr__()[:60])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converged in 17 iterations.\n"
     ]
    }
   ],
   "source": [
    "maze_v2 = MazeMDP(maze_grid = maze_grid, gamma  =  0.9)\n",
    "opt_vf_v2, opt_policy_v2 = value_iteration_result(mdp = maze_v2, gamma = maze_v2.gamma, print_steps = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{(0, 0): 0.2058911320946491, (0, 2): 0.31381059609000017, (0\n"
     ]
    }
   ],
   "source": [
    "print(opt_vf_v2.__repr__()[:60])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Remark : setting $\\gamma$ to a low value (e.g 0.1) reduces the number of iterations to 7, but the Value function becomes so close to zero that it is likely float approximations errors are occuring."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The two methods converge in 17 iterations, and they produce (as they should) the same deterministic policies.\n",
    "The below cell checks the last claim."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def same_policy(pi_1 : FinitePolicy, pi_2 : FinitePolicy) -> bool:\n",
    "    return all([pi_1.states() == pi_2.states()] +\\\n",
    "               [(pi_1.act(s) is None and  pi_2.act(s) is None) or\\\n",
    "                (pi_1.act(s).table == pi_2.act(s).table) for s in pi_1.states()])\n",
    "same_policy(opt_policy_v2, opt_policy_v1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 2: MRP Value Function Approximation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We know that the Value function (denoted V) can be characterized as the only fixed point of the Bellman operator (Banach fixed-point Theorem).\n",
    "Let $\\Phi$ be a feature matrix, the following equivalences hold:\n",
    "\n",
    "We can approximate exactly $V$  \n",
    "$\\Leftrightarrow \\exists \\beta \\in R^n, ~ V = \\Phi \\beta $  \n",
    "$\\Leftrightarrow \\exists \\beta \\in R^n, ~ \\Phi \\beta = \\mathcal{R} + \\gamma \\mathcal{P}\\Phi \\beta $  \n",
    "$\\Leftrightarrow \\exists \\beta \\in R^n, ~ \\Phi \\beta =  ( \\mathbf{I} - \\gamma \\mathcal{P}\\Phi )^{-1} \\mathcal{R} $  \n",
    "$\\Leftrightarrow ( \\mathbf{I} - \\gamma \\mathcal{P}\\Phi )^{-1} \\mathcal{R} \\in \\text{Range}(\\Phi)$\n",
    "\n",
    "\n",
    "\n",
    "Therefore, a minimal condition for V to be exactly linearly approximated with the design matrix $\\Phi$ is that $( \\mathbf{I} - \\gamma \\mathcal{P}\\Phi )^{-1} \\mathcal{R}$ should be in the Range of $\\Phi$\n",
    "\n",
    "\n",
    "Remark : let $\\gamma = 0$, this condition reduces to the existence of a perfect linear relationship between $\\Phi$ and $\\mathcal{R}$ : $\\mathcal{R} = \\Phi \\beta$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 3: Career Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In what follows, $l$ and $s$ always denote non-negative integers. $\\alpha$ is a fixed postive number.\n",
    "\n",
    "* The state space is the wage-space : $\\mathcal{S} = \\{1, ~...~,~ W\\}$, with no terminating state.\n",
    "* The action space is the schedule-space : $\\mathcal{A} = \\{(l,s), ~ 0 \\leq l + s \\leq H\\}$\n",
    "* $\\forall w \\in \\mathcal{S},~ \\forall (l,s) \\in \\mathcal{A}$:  \n",
    "\n",
    "$ \\text{if} ~ w < W, ~ \\mathcal{P}(w,(l,s), w) = (1 - \\frac{\\beta s}{H})e^{-\\alpha l}$  \n",
    "\n",
    "$ \\text{if} ~ w + 1 < W, ~ \\mathcal{P}(w,(l,s), w + 1) = (\\frac{\\beta s}{H} + \\alpha l)e^{-\\alpha l} $  \n",
    "\n",
    "$  \\forall x \\in \\mathbb{Z}_{\\geq 2},~\\text{if} ~ w + x < W,  ~ \\mathcal{P}(w,(l,s), w + x) = \\frac{(\\alpha l)^xe^{-\\alpha l}}{x!}$  \n",
    "\n",
    "$\\mathcal{P}(w,(l,s), W) = 1 - \\sum_{0 \\leq x \\leq W-w -1} \\mathcal{P}(w,(l,s), w+x)$  \n",
    "\n",
    "\n",
    "* $\\forall w \\in \\mathcal{S},~ \\forall (l,s) \\in \\mathcal{A} ,~ \\mathcal{R}_T(w, (l,s))= w (H - l - s)$\n",
    "*  $0 \\leq \\gamma < 1$, for example $\\gamma = 0.5^{1/30} \\approx 0.977$ would correspond to giving a 50% discount after exactly one month."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rl.distribution import Categorical\n",
    "from numpy.random import randint\n",
    "from math import exp\n",
    "Wage = int\n",
    "DayPlan = Tuple[int, int]\n",
    "\n",
    "#remark : we will use b (book) in place of l (learn) to avoid confusion with the number 1.\n",
    "\n",
    "class CareerMDP(FiniteMarkovDecisionProcess):\n",
    "    def __init__(self, alpha : float = 0.08, beta : float = 0.82 ,W : int = 30, H:int = 10):\n",
    "        # pre-computation of factorials\n",
    "        factorial = [1]\n",
    "        for i in range(1,W+1):\n",
    "            factorial.append(factorial[-1]*i)\n",
    "            \n",
    "        def proba(a : DayPlan, x : int) -> float:\n",
    "            b,s = a\n",
    "            \"\"\"returns the probability of going from w to w+x with w+x < W\"\"\"\n",
    "            res = (alpha * b)**x * exp(-alpha*b) / factorial[x]\n",
    "            if x == 0:\n",
    "                res *= (1-(beta * s / H))\n",
    "            elif x== 1:\n",
    "                res += (beta * s / H)*exp(-alpha*b)\n",
    "            return res\n",
    "        \n",
    "        sa_mapping : StateActionMapping[Wage, DayPlan, Wage] = {}\n",
    "        for w in range(1,W+1):\n",
    "            sa_mapping[w] = {}\n",
    "            for b in range(H+1):\n",
    "                for s in range(H - b):\n",
    "                    #state-reward probabilities\n",
    "                    proba_dict = {(w+x, (H-b-s)*w): proba((b,s),x) for x in range(W-w)\\\n",
    "                                  if proba((b,s),x) > 0}\n",
    "                    total = sum(proba_dict.values())\n",
    "                    if total < 1. :\n",
    "                        proba_dict[(W, (H-b-s)*w)] =  (1. - total)\n",
    "                    sa_mapping[w][(b,s)] = Categorical(proba_dict)\n",
    "        super().__init__(sa_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "career_mdp = CareerMDP(W = 30, H = 10, alpha = 0.08, beta = 0.82)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converged in 337 iterations.\n"
     ]
    }
   ],
   "source": [
    "vf, pi = value_iteration_result(mdp = career_mdp, gamma = 0.95, print_steps = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The optimal policy is :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "For State 1:\n",
       "  Do Action (0, 9) with Probability 1.000\n",
       "For State 2:\n",
       "  Do Action (0, 9) with Probability 1.000\n",
       "For State 3:\n",
       "  Do Action (0, 9) with Probability 1.000\n",
       "For State 4:\n",
       "  Do Action (0, 9) with Probability 1.000\n",
       "For State 5:\n",
       "  Do Action (0, 9) with Probability 1.000\n",
       "For State 6:\n",
       "  Do Action (0, 9) with Probability 1.000\n",
       "For State 7:\n",
       "  Do Action (0, 9) with Probability 1.000\n",
       "For State 8:\n",
       "  Do Action (0, 9) with Probability 1.000\n",
       "For State 9:\n",
       "  Do Action (0, 9) with Probability 1.000\n",
       "For State 10:\n",
       "  Do Action (0, 9) with Probability 1.000\n",
       "For State 11:\n",
       "  Do Action (0, 9) with Probability 1.000\n",
       "For State 12:\n",
       "  Do Action (0, 9) with Probability 1.000\n",
       "For State 13:\n",
       "  Do Action (0, 9) with Probability 1.000\n",
       "For State 14:\n",
       "  Do Action (0, 9) with Probability 1.000\n",
       "For State 15:\n",
       "  Do Action (0, 9) with Probability 1.000\n",
       "For State 16:\n",
       "  Do Action (0, 0) with Probability 1.000\n",
       "For State 17:\n",
       "  Do Action (0, 0) with Probability 1.000\n",
       "For State 18:\n",
       "  Do Action (0, 0) with Probability 1.000\n",
       "For State 19:\n",
       "  Do Action (0, 0) with Probability 1.000\n",
       "For State 20:\n",
       "  Do Action (0, 0) with Probability 1.000\n",
       "For State 21:\n",
       "  Do Action (0, 0) with Probability 1.000\n",
       "For State 22:\n",
       "  Do Action (0, 0) with Probability 1.000\n",
       "For State 23:\n",
       "  Do Action (0, 0) with Probability 1.000\n",
       "For State 24:\n",
       "  Do Action (0, 0) with Probability 1.000\n",
       "For State 25:\n",
       "  Do Action (0, 0) with Probability 1.000\n",
       "For State 26:\n",
       "  Do Action (0, 0) with Probability 1.000\n",
       "For State 27:\n",
       "  Do Action (0, 0) with Probability 1.000\n",
       "For State 28:\n",
       "  Do Action (0, 0) with Probability 1.000\n",
       "For State 29:\n",
       "  Do Action (0, 0) with Probability 1.000\n",
       "For State 30:\n",
       "  Do Action (0, 0) with Probability 1.000"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The optimal policy is to spend 100% of one's time searching for a new job if he isn't paid at least 16, and to work fulltime otherwise. The rationale behind this is that since alpha is very low, the discounted expectation  of raise from searching is larger. As long as the expected raise from a search time is greater than one's salary, he prefers to study fulltime. As soon as it's not the case there is no incentive to study anymore.\n",
    "\n",
    "It is pretty sensible that either searching or learning is strictly better than the other (it depends on alpha, cranking it up will replace the role of s with l).\n",
    "To get the right intuition on the actual policy let's study the two extreme cases.\n",
    "* Hungry worker : is only interested in today's pay ($\\gamma = 0$) : will always work (l = s = 0)\n",
    "* Disinterested worker : is only interested in maximizing daily income, but doesn't care to wait as long as necessary : will always spend all his time getting to the next salary.\n",
    "\n",
    "In our case, it is a tradeoff between the two extremes, where the agent is willing to increase its wage, up until the time spent increasing that is not worth the salary loss with regards to his patience.\n",
    "\n",
    "This is a pretty short sighted strategy, as obviously in the (very) long run it is worth it to spend the first 30 days getting to W. Let's test that guess by setting $\\gamma = 0.99$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "For State 1:\n",
       "  Do Action (0, 9) with Probability 1.000\n",
       "For State 2:\n",
       "  Do Action (0, 9) with Probability 1.000\n",
       "For State 3:\n",
       "  Do Action (0, 9) with Probability 1.000\n",
       "For State 4:\n",
       "  Do Action (0, 9) with Probability 1.000\n",
       "For State 5:\n",
       "  Do Action (0, 9) with Probability 1.000\n",
       "For State 6:\n",
       "  Do Action (0, 9) with Probability 1.000\n",
       "For State 7:\n",
       "  Do Action (0, 9) with Probability 1.000\n",
       "For State 8:\n",
       "  Do Action (0, 9) with Probability 1.000\n",
       "For State 9:\n",
       "  Do Action (0, 9) with Probability 1.000\n",
       "For State 10:\n",
       "  Do Action (0, 9) with Probability 1.000\n",
       "For State 11:\n",
       "  Do Action (0, 9) with Probability 1.000\n",
       "For State 12:\n",
       "  Do Action (0, 9) with Probability 1.000\n",
       "For State 13:\n",
       "  Do Action (0, 9) with Probability 1.000\n",
       "For State 14:\n",
       "  Do Action (0, 9) with Probability 1.000\n",
       "For State 15:\n",
       "  Do Action (0, 9) with Probability 1.000\n",
       "For State 16:\n",
       "  Do Action (0, 9) with Probability 1.000\n",
       "For State 17:\n",
       "  Do Action (0, 9) with Probability 1.000\n",
       "For State 18:\n",
       "  Do Action (0, 9) with Probability 1.000\n",
       "For State 19:\n",
       "  Do Action (0, 9) with Probability 1.000\n",
       "For State 20:\n",
       "  Do Action (0, 9) with Probability 1.000\n",
       "For State 21:\n",
       "  Do Action (0, 9) with Probability 1.000\n",
       "For State 22:\n",
       "  Do Action (0, 9) with Probability 1.000\n",
       "For State 23:\n",
       "  Do Action (0, 9) with Probability 1.000\n",
       "For State 24:\n",
       "  Do Action (0, 9) with Probability 1.000\n",
       "For State 25:\n",
       "  Do Action (0, 9) with Probability 1.000\n",
       "For State 26:\n",
       "  Do Action (0, 9) with Probability 1.000\n",
       "For State 27:\n",
       "  Do Action (0, 9) with Probability 1.000\n",
       "For State 28:\n",
       "  Do Action (0, 9) with Probability 1.000\n",
       "For State 29:\n",
       "  Do Action (0, 9) with Probability 1.000\n",
       "For State 30:\n",
       "  Do Action (0, 0) with Probability 1.000"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vf, pi = value_iteration_result(mdp = career_mdp, gamma = 0.99);pi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected, the agent now is willing to take the effort to rise to the top.\n",
    "Let's verify that increasing alpha will change the roles of l and s:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converged in 337 iterations.\n"
     ]
    }
   ],
   "source": [
    "career_mdp = CareerMDP(W = 30, H = 10, alpha = 0.16, beta = 0.82)\n",
    "vf, pi = value_iteration_result(mdp = career_mdp, gamma = 0.95, print_steps = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "For State 1:\n",
       "  Do Action (9, 0) with Probability 1.000\n",
       "For State 2:\n",
       "  Do Action (9, 0) with Probability 1.000\n",
       "For State 3:\n",
       "  Do Action (9, 0) with Probability 1.000\n",
       "For State 4:\n",
       "  Do Action (9, 0) with Probability 1.000\n",
       "For State 5:\n",
       "  Do Action (9, 0) with Probability 1.000\n",
       "For State 6:\n",
       "  Do Action (9, 0) with Probability 1.000\n",
       "For State 7:\n",
       "  Do Action (9, 0) with Probability 1.000\n",
       "For State 8:\n",
       "  Do Action (9, 0) with Probability 1.000\n",
       "For State 9:\n",
       "  Do Action (9, 0) with Probability 1.000\n",
       "For State 10:\n",
       "  Do Action (9, 0) with Probability 1.000\n",
       "For State 11:\n",
       "  Do Action (9, 0) with Probability 1.000\n",
       "For State 12:\n",
       "  Do Action (9, 0) with Probability 1.000\n",
       "For State 13:\n",
       "  Do Action (9, 0) with Probability 1.000\n",
       "For State 14:\n",
       "  Do Action (9, 0) with Probability 1.000\n",
       "For State 15:\n",
       "  Do Action (9, 0) with Probability 1.000\n",
       "For State 16:\n",
       "  Do Action (9, 0) with Probability 1.000\n",
       "For State 17:\n",
       "  Do Action (9, 0) with Probability 1.000\n",
       "For State 18:\n",
       "  Do Action (9, 0) with Probability 1.000\n",
       "For State 19:\n",
       "  Do Action (9, 0) with Probability 1.000\n",
       "For State 20:\n",
       "  Do Action (9, 0) with Probability 1.000\n",
       "For State 21:\n",
       "  Do Action (9, 0) with Probability 1.000\n",
       "For State 22:\n",
       "  Do Action (9, 0) with Probability 1.000\n",
       "For State 23:\n",
       "  Do Action (9, 0) with Probability 1.000\n",
       "For State 24:\n",
       "  Do Action (9, 0) with Probability 1.000\n",
       "For State 25:\n",
       "  Do Action (9, 0) with Probability 1.000\n",
       "For State 26:\n",
       "  Do Action (9, 0) with Probability 1.000\n",
       "For State 27:\n",
       "  Do Action (5, 0) with Probability 1.000\n",
       "For State 28:\n",
       "  Do Action (2, 0) with Probability 1.000\n",
       "For State 29:\n",
       "  Do Action (0, 0) with Probability 1.000\n",
       "For State 30:\n",
       "  Do Action (0, 0) with Probability 1.000"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected the agent now only Learns instead of searching, with a slight difference : when approaching W, he spends less time learning and some time working. This is because the learning action is stochastic with a tail collapse at W, and a mode at 0, which makes the expected reward from learning non-linear with the time spent and the distance to W."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
