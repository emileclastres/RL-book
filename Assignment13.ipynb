{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 13 : RL for Control"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) Tabular MC for Control"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Callable, Iterable, Iterator, TypeVar, Tuple\n",
    "from itertools import islice\n",
    "from rl.function_approx import FunctionApprox\n",
    "import rl.markov_process as mp\n",
    "import rl.markov_decision_process as mdp\n",
    "import rl.iterate as iterate\n",
    "\n",
    "S = TypeVar('S')\n",
    "from typing import Iterable, Iterator, Tuple, TypeVar, Callable, Mapping\n",
    "from dataclasses import dataclass\n",
    "from rl.distribution import Distribution, Choose, Categorical\n",
    "from rl.function_approx import FunctionApprox\n",
    "import rl.markov_process as mp\n",
    "import rl.markov_decision_process as markov_decision_process\n",
    "from rl.markov_decision_process import (MarkovDecisionProcess, FiniteMarkovDecisionProcess)\n",
    "from rl.returns import returns\n",
    "\n",
    "S = TypeVar('S')\n",
    "A = TypeVar('A')\n",
    "\n",
    "import operator\n",
    "\n",
    "\n",
    "def mc_tabular_control(\n",
    "        mdp : FiniteMarkovDecisionProcess[S,A],\n",
    "        states: Distribution[S],\n",
    "        γ: float,\n",
    "        f : Callable[[int], float] = lambda n : 1/n,\n",
    "        tolerance: float = 1e-6\n",
    "):\n",
    "    epsilon = 1\n",
    "    state_space = [s for s in mdp.states()]\n",
    "    Qtable   = {s:{a:1/len(mdp.actions(s)) for a in mdp.actions(s)} for s in state_space}\n",
    "    counts  = {s:{a:0 for a in mdp.actions(s)} for s in state_space}\n",
    "    actions_space = {s:[a for a in mdp.actions(s)] for s in state_space}\n",
    "    def make_epsilon_greedy(Qtable, epsilon):\n",
    "        egreedy = {s:{} for s in state_space}\n",
    "        for state in state_space:\n",
    "            greedy_action = max(Qtable[state].items(), key = operator.itemgetter(1))[0]\n",
    "            As = actions_space[state]\n",
    "            egreedy[state]  = Categorical({a: epsilon/len(As) + 1- epsilon if a == greedy_action else epsilon/len(As) for a in As})\n",
    "        return egreedy\n",
    "    \n",
    "    def make_greedy(Qtable):\n",
    "        greedy = {s:{} for s in state_space}\n",
    "        for state in state_space:\n",
    "            greedy_action = max(Qtable[state].items(), key = operator.itemgetter(1))[0]\n",
    "            greedy[state]  = Constant(greedy_action)\n",
    "        return markov_decision_process.FinitePolicy(greedy)\n",
    "    \n",
    "    p = markov_decision_process.FinitePolicy(make_epsilon_greedy(Qtable, 1))\n",
    "    k=0\n",
    "    while True:\n",
    "        k+=1\n",
    "        epsilon = 1/k\n",
    "        trace: Iterable[markov_decision_process.TransitionStep[S, A]] =\\\n",
    "            mdp.simulate_actions(states, p)\n",
    "        trace_rewards = returns(trace, γ, tolerance)\n",
    "        for step in trace_rewards:\n",
    "            s,a = (step.state,step.action)\n",
    "            counts[s][a] = counts[s].get(a, 0) + 1\n",
    "            if counts[s][a] == 1:\n",
    "                Qtable[s][a] = step.return_\n",
    "            else:\n",
    "                Qtable[s][a] = step.return_ * f(counts[s][a])+(1-f(counts[s][a]))*Qtable[s].get(a, 0.)\n",
    "\n",
    "        p = markov_decision_process.FinitePolicy(make_epsilon_greedy(Qtable, epsilon))\n",
    "        \n",
    "        yield (Qtable,make_greedy(Qtable))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal Policy\n",
      "--------------\n",
      "For State InventoryState(on_hand=0, on_order=0):\n",
      "  Do Action 1 with Probability 1.000\n",
      "For State InventoryState(on_hand=0, on_order=1):\n",
      "  Do Action 1 with Probability 1.000\n",
      "For State InventoryState(on_hand=0, on_order=2):\n",
      "  Do Action 0 with Probability 1.000\n",
      "For State InventoryState(on_hand=1, on_order=0):\n",
      "  Do Action 1 with Probability 1.000\n",
      "For State InventoryState(on_hand=1, on_order=1):\n",
      "  Do Action 0 with Probability 1.000\n",
      "For State InventoryState(on_hand=2, on_order=0):\n",
      "  Do Action 0 with Probability 1.000\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from dataclasses import dataclass\n",
    "from typing import Tuple, Dict\n",
    "from rl.markov_decision_process import FiniteMarkovDecisionProcess\n",
    "from rl.markov_decision_process import FinitePolicy, StateActionMapping\n",
    "from rl.markov_process import FiniteMarkovProcess, FiniteMarkovRewardProcess\n",
    "from rl.distribution import Categorical, Constant\n",
    "from scipy.stats import poisson\n",
    "\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class InventoryState:\n",
    "    on_hand: int\n",
    "    on_order: int\n",
    "\n",
    "    def inventory_position(self) -> int:\n",
    "        return self.on_hand + self.on_order\n",
    "\n",
    "\n",
    "InvOrderMapping = StateActionMapping[InventoryState, int]\n",
    "\n",
    "\n",
    "class SimpleInventoryMDPCap(FiniteMarkovDecisionProcess[InventoryState, int]):\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        capacity: int,\n",
    "        poisson_lambda: float,\n",
    "        holding_cost: float,\n",
    "        stockout_cost: float\n",
    "    ):\n",
    "        self.capacity: int = capacity\n",
    "        self.poisson_lambda: float = poisson_lambda\n",
    "        self.holding_cost: float = holding_cost\n",
    "        self.stockout_cost: float = stockout_cost\n",
    "\n",
    "        self.poisson_distr = poisson(poisson_lambda)\n",
    "        super().__init__(self.get_action_transition_reward_map())\n",
    "\n",
    "    def get_action_transition_reward_map(self) -> InvOrderMapping:\n",
    "        d: Dict[InventoryState, Dict[int, Categorical[Tuple[InventoryState,\n",
    "                                                            float]]]] = {}\n",
    "\n",
    "        for alpha in range(self.capacity + 1):\n",
    "            for beta in range(self.capacity + 1 - alpha):\n",
    "                state: InventoryState = InventoryState(alpha, beta)\n",
    "                ip: int = state.inventory_position()\n",
    "                base_reward: float = - self.holding_cost * alpha\n",
    "                d1: Dict[int, Categorical[Tuple[InventoryState, float]]] = {}\n",
    "\n",
    "                for order in range(self.capacity - ip + 1):\n",
    "                    sr_probs_dict: Dict[Tuple[InventoryState, float], float] =\\\n",
    "                        {(InventoryState(ip - i, order), base_reward):\n",
    "                         self.poisson_distr.pmf(i) for i in range(ip)}\n",
    "\n",
    "                    probability: float = 1 - self.poisson_distr.cdf(ip - 1)\n",
    "                    reward: float = base_reward - self.stockout_cost *\\\n",
    "                        (probability * (self.poisson_lambda - ip) +\n",
    "                         ip * self.poisson_distr.pmf(ip))\n",
    "                    sr_probs_dict[(InventoryState(0, order), reward)] = \\\n",
    "                        probability\n",
    "                    d1[order] = Categorical(sr_probs_dict)\n",
    "\n",
    "                d[state] = d1\n",
    "        return d\n",
    "\n",
    "\n",
    "if True:\n",
    "    from pprint import pprint\n",
    "\n",
    "    user_capacity = 2\n",
    "    user_poisson_lambda = 1.0\n",
    "    user_holding_cost = 1.0\n",
    "    user_stockout_cost = 10.0\n",
    "\n",
    "    user_gamma = 0.9\n",
    "\n",
    "    si_mdp: FiniteMarkovDecisionProcess[InventoryState, int] =\\\n",
    "        SimpleInventoryMDPCap(\n",
    "            capacity=user_capacity,\n",
    "            poisson_lambda=user_poisson_lambda,\n",
    "            holding_cost=user_holding_cost,\n",
    "            stockout_cost=user_stockout_cost\n",
    "        )\n",
    "\n",
    "    \n",
    "\n",
    "    print(\"Optimal Policy\")\n",
    "    print(\"--------------\")\n",
    "    opt_vf_vi, opt_policy_vi = value_iteration_result(si_mdp, gamma=user_gamma)\n",
    "    print(opt_policy_vi)\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "For State InventoryState(on_hand=0, on_order=0):\n",
       "  Do Action 1 with Probability 1.000\n",
       "For State InventoryState(on_hand=0, on_order=1):\n",
       "  Do Action 1 with Probability 1.000\n",
       "For State InventoryState(on_hand=0, on_order=2):\n",
       "  Do Action 0 with Probability 1.000\n",
       "For State InventoryState(on_hand=1, on_order=0):\n",
       "  Do Action 1 with Probability 1.000\n",
       "For State InventoryState(on_hand=1, on_order=1):\n",
       "  Do Action 0 with Probability 1.000\n",
       "For State InventoryState(on_hand=2, on_order=0):\n",
       "  Do Action 0 with Probability 1.000"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Qtables_policies = mc_tabular_control(\n",
    "        si_mdp,\n",
    "        Constant(InventoryState(on_hand=0, on_order=0)),\n",
    "        user_gamma,\n",
    "        lambda n : 1/n,\n",
    "        1e-6)\n",
    "\n",
    "*_, (Qtable, mc_policy) = islice(Qtables_policies, 100)\n",
    "mc_policy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Success ! we learn the same optimal policy !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mc_control_GLIE_1k(\n",
    "        mdp: MarkovDecisionProcess[S, A],\n",
    "        states: Distribution[S],\n",
    "        approx_0: FunctionApprox[Tuple[S, A]],\n",
    "        γ: float,\n",
    "        tolerance: float = 1e-6\n",
    ") -> Iterator[FunctionApprox[Tuple[S, A]]]:\n",
    "    '''Evaluate an MRP using the monte carlo method, simulating episodes\n",
    "    of the given number of steps.\n",
    "\n",
    "    Each value this function yields represents the approximated value\n",
    "    function for the MRP after one additional epsiode.\n",
    "\n",
    "    Arguments:\n",
    "      mrp -- the Markov Reward Process to evaluate\n",
    "      states -- distribution of states to start episodes from\n",
    "      approx_0 -- initial approximation of value function\n",
    "      γ -- discount rate (0 < γ ≤ 1)\n",
    "      ϵ -- the fraction of the actions where we explore rather\n",
    "      than following the optimal policy\n",
    "      tolerance -- a small value—we stop iterating once γᵏ ≤ tolerance\n",
    "\n",
    "    Returns an iterator with updates to the approximated Q function\n",
    "    after each episode.\n",
    "\n",
    "    '''\n",
    "    ϵ,k = 1,0\n",
    "    q = approx_0\n",
    "    p = markov_decision_process.policy_from_q(q, mdp, ϵ)\n",
    "\n",
    "    while True:\n",
    "        ϵ,k = 1/k,k+1\n",
    "        trace: Iterable[markov_decision_process.TransitionStep[S, A]] =\\\n",
    "            mdp.simulate_actions(states, p)\n",
    "        q = q.update(\n",
    "            ((step.state, step.action), step.return_)\n",
    "            for step in returns(trace, γ, tolerance)\n",
    "        )\n",
    "        p = markov_decision_process.policy_from_q(q, mdp, ϵ)\n",
    "        yield q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-90-66417c96a72c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    251\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    252\u001b[0m     \u001b[0mit_qvf\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIterator\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mDNNApprox\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mTuple\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 253\u001b[0;31m         \u001b[0maad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward_induction_qvf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    254\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    255\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Backward Induction on Q-Value Function\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-90-66417c96a72c>\u001b[0m in \u001b[0;36mbackward_induction_qvf\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    133\u001b[0m             \u001b[0mγ\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1.0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m             \u001b[0mnum_state_samples\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_state_samples\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 135\u001b[0;31m             \u001b[0merror_tolerance\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merror_tolerance\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    136\u001b[0m         )\n\u001b[1;32m    137\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/CME241/RL-book/rl/approximate_dynamic_programming.py\u001b[0m in \u001b[0;36mback_opt_qvf\u001b[0;34m(mdp_f0_mu_triples, γ, num_state_samples, error_tolerance)\u001b[0m\n\u001b[1;32m    329\u001b[0m         this_qvf = approx0.solve(\n\u001b[1;32m    330\u001b[0m             [((s, a), mdp.step(s, a).expectation(return_))\n\u001b[0;32m--> 331\u001b[0;31m              \u001b[0;32mfor\u001b[0m \u001b[0ms\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmu\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample_n\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_state_samples\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    332\u001b[0m              if not mdp.is_terminal(s) for a in mdp.actions(s)],\n\u001b[1;32m    333\u001b[0m             \u001b[0merror_tolerance\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/CME241/RL-book/rl/approximate_dynamic_programming.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    330\u001b[0m             [((s, a), mdp.step(s, a).expectation(return_))\n\u001b[1;32m    331\u001b[0m              \u001b[0;32mfor\u001b[0m \u001b[0ms\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmu\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample_n\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_state_samples\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 332\u001b[0;31m              if not mdp.is_terminal(s) for a in mdp.actions(s)],\n\u001b[0m\u001b[1;32m    333\u001b[0m             \u001b[0merror_tolerance\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    334\u001b[0m         )\n",
      "\u001b[0;32m~/Documents/CME241/RL-book/rl/distribution.py\u001b[0m in \u001b[0;36mexpectation\u001b[0;34m(self, f)\u001b[0m\n\u001b[1;32m     90\u001b[0m         '''\n\u001b[1;32m     91\u001b[0m         return sum(f(self.sample()) for _ in\n\u001b[0;32m---> 92\u001b[0;31m                    range(self.expectation_samples)) / self.expectation_samples\n\u001b[0m\u001b[1;32m     93\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/CME241/RL-book/rl/distribution.py\u001b[0m in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     89\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m         '''\n\u001b[0;32m---> 91\u001b[0;31m         return sum(f(self.sample()) for _ in\n\u001b[0m\u001b[1;32m     92\u001b[0m                    range(self.expectation_samples)) / self.expectation_samples\n\u001b[1;32m     93\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/CME241/RL-book/rl/approximate_dynamic_programming.py\u001b[0m in \u001b[0;36mreturn_\u001b[0;34m(s_r, i)\u001b[0m\n\u001b[1;32m    323\u001b[0m                     for a in mdp_f0_mu_triples[horizon - i][0].actions(s1))\n\u001b[1;32m    324\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 325\u001b[0;31m                 \u001b[0;32mnot\u001b[0m \u001b[0mmdp_f0_mu_triples\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnum_steps\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_terminal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    326\u001b[0m                 \u001b[0;32melse\u001b[0m \u001b[0;36m0.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    327\u001b[0m             )\n",
      "\u001b[0;32m~/Documents/CME241/RL-book/rl/approximate_dynamic_programming.py\u001b[0m in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    321\u001b[0m             return r + γ * (\n\u001b[1;32m    322\u001b[0m                 max(qvf[i-1].evaluate([(s1, a)]).item()\n\u001b[0;32m--> 323\u001b[0;31m                     for a in mdp_f0_mu_triples[horizon - i][0].actions(s1))\n\u001b[0m\u001b[1;32m    324\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    325\u001b[0m                 \u001b[0;32mnot\u001b[0m \u001b[0mmdp_f0_mu_triples\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnum_steps\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_terminal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/CME241/RL-book/rl/function_approx.py\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(self, x_values_seq)\u001b[0m\n\u001b[1;32m    598\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    599\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_values_seq\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIterable\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 600\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward_propagation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_values_seq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    601\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    602\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwithin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mother\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mFunctionApprox\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtolerance\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/CME241/RL-book/rl/function_approx.py\u001b[0m in \u001b[0;36mforward_propagation\u001b[0;34m(self, x_values_seq)\u001b[0m\n\u001b[1;32m    592\u001b[0m         ret.append(\n\u001b[1;32m    593\u001b[0m             self.dnn_spec.output_activation(\n\u001b[0;32m--> 594\u001b[0;31m                 \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    595\u001b[0m             )[:, 0]\n\u001b[1;32m    596\u001b[0m         )\n",
      "\u001b[0;32m<ipython-input-90-66417c96a72c>\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m    216\u001b[0m         \u001b[0mhidden_activation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    217\u001b[0m         \u001b[0mhidden_activation_deriv\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mones_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 218\u001b[0;31m         \u001b[0moutput_activation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msign\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    219\u001b[0m         \u001b[0moutput_activation_deriv\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    220\u001b[0m     )\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from dataclasses import dataclass\n",
    "from typing import Sequence, Callable, Tuple, Iterator\n",
    "from rl.distribution import Distribution, SampledDistribution, Choose, Gaussian\n",
    "from rl.markov_decision_process import MarkovDecisionProcess, Policy\n",
    "from rl.function_approx import DNNSpec, AdamGradient, DNNApprox\n",
    "from rl.approximate_dynamic_programming import back_opt_vf_and_policy\n",
    "from rl.approximate_dynamic_programming import back_opt_qvf\n",
    "from operator import itemgetter\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class AssetAllocDiscrete:\n",
    "    risky_return_distributions: Sequence[Distribution[float]]\n",
    "    riskless_returns: Sequence[float]\n",
    "    utility_func: Callable[[float], float]\n",
    "    risky_alloc_choices: Sequence[float]\n",
    "    feature_functions: Sequence[Callable[[Tuple[float, float]], float]]\n",
    "    dnn_spec: DNNSpec\n",
    "    initial_wealth_distribution: Distribution[float]\n",
    "    all_gaussian : bool  = False\n",
    "    def time_steps(self) -> int:\n",
    "        return len(self.risky_return_distributions)\n",
    "\n",
    "    def uniform_actions(self) -> Choose[float]:\n",
    "        return Choose(set(self.risky_alloc_choices))\n",
    "\n",
    "    def get_mdp(self, t: int) -> MarkovDecisionProcess[float, float]:\n",
    "        \"\"\"\n",
    "        State is Wealth W_t, Action is investment in risky asset (= x_t)\n",
    "        Investment in riskless asset is W_t - x_t\n",
    "        \"\"\"\n",
    "\n",
    "        distr: Distribution[float] = self.risky_return_distributions[t]\n",
    "        rate: float = self.riskless_returns[t]\n",
    "        alloc_choices: Sequence[float] = self.risky_alloc_choices\n",
    "        steps: int = self.time_steps()\n",
    "        utility_f: Callable[[float], float] = self.utility_func\n",
    "\n",
    "        class AssetAllocMDP(MarkovDecisionProcess[float, float]):\n",
    "\n",
    "            def step(\n",
    "                self,\n",
    "                wealth: float,\n",
    "                alloc: float\n",
    "            ) -> SampledDistribution[Tuple[float, float]]:\n",
    "\n",
    "                def sr_sampler_func(\n",
    "                    wealth=wealth,\n",
    "                    alloc=alloc\n",
    "                ) -> Tuple[float, float]:\n",
    "                    next_wealth: float = alloc * (1 + distr.sample()) \\\n",
    "                        + (wealth - alloc) * (1 + rate)\n",
    "                    reward: float = utility_f(next_wealth) \\\n",
    "                        if t == steps - 1 else 0.\n",
    "                    return (next_wealth, reward)\n",
    "\n",
    "                return SampledDistribution(\n",
    "                    sampler=sr_sampler_func,\n",
    "                    expectation_samples=1000\n",
    "                )\n",
    "\n",
    "            def actions(self, wealth: float) -> Sequence[float]:\n",
    "                return alloc_choices\n",
    "\n",
    "        return AssetAllocMDP()\n",
    "\n",
    "    def get_qvf_func_approx(self) -> DNNApprox[Tuple[float, float]]:\n",
    "\n",
    "        adam_gradient: AdamGradient = AdamGradient(\n",
    "            learning_rate=0.1,\n",
    "            decay1=0.9,\n",
    "            decay2=0.999\n",
    "        )\n",
    "        return DNNApprox.create(\n",
    "            feature_functions=self.feature_functions,\n",
    "            dnn_spec=self.dnn_spec,\n",
    "            adam_gradient=adam_gradient\n",
    "        )\n",
    "\n",
    "\n",
    "    def get_states_distribution(self, t: int) -> SampledDistribution[float]:\n",
    "        if self.all_gaussian:\n",
    "            actions_distr: Choose[float] = self.uniform_actions()\n",
    "    \n",
    "            wealth: float = self.initial_wealth_distribution\n",
    "            μ, σ = wealth.μ, wealth.σ\n",
    "            for i in range(t):\n",
    "                distr: Distribution[float] = self.risky_return_distributions[i]\n",
    "                rate: float = self.riskless_returns[i]\n",
    "                alloc: float = actions_distr.sample()\n",
    "                μ = alloc * (1 + distr.μ) + (μ - alloc)*(1+rate)\n",
    "                #assuming independance : \n",
    "                σ = np.abs(alloc) * distr.σ + (σ) * np.abs(1 + rate)\n",
    "                    \n",
    "            return Gaussian(μ, σ)\n",
    "        else:\n",
    "            actions_distr: Choose[float] = self.uniform_actions()\n",
    "\n",
    "            def states_sampler_func() -> float:\n",
    "                wealth: float = self.initial_wealth_distribution.sample()\n",
    "                for i in range(t):\n",
    "                    distr: Distribution[float] = self.risky_return_distributions[i]\n",
    "                    rate: float = self.riskless_returns[i]\n",
    "                    alloc: float = actions_distr.sample()\n",
    "                    wealth = alloc * (1 + distr.sample()) + \\\n",
    "                        (wealth - alloc) * (1 + rate)\n",
    "                return wealth\n",
    "    \n",
    "            return SampledDistribution(states_sampler_func)\n",
    "\n",
    "\n",
    "    def backward_induction_qvf(self) -> \\\n",
    "            Iterator[DNNApprox[Tuple[float, float]]]:\n",
    "\n",
    "        init_fa: DNNApprox[Tuple[float, float]] = self.get_qvf_func_approx()\n",
    "\n",
    "        mdp_f0_mu_triples: Sequence[Tuple[\n",
    "            MarkovDecisionProcess[float, float],\n",
    "            DNNApprox[Tuple[float, float]],\n",
    "            SampledDistribution[float]\n",
    "        ]] = [(\n",
    "            self.get_mdp(i),\n",
    "            init_fa,\n",
    "            self.get_states_distribution(i)\n",
    "        ) for i in range(self.time_steps())]\n",
    "\n",
    "        num_state_samples: int = 300\n",
    "        error_tolerance: float = 1e-6\n",
    "\n",
    "        return back_opt_qvf(\n",
    "            mdp_f0_mu_triples=mdp_f0_mu_triples,\n",
    "            γ=1.0,\n",
    "            num_state_samples=num_state_samples,\n",
    "            error_tolerance=error_tolerance\n",
    "        )\n",
    "\n",
    "    def get_vf_func_approx(\n",
    "        self,\n",
    "        ff: Sequence[Callable[[float], float]]\n",
    "    ) -> DNNApprox[float]:\n",
    "\n",
    "        adam_gradient: AdamGradient = AdamGradient(\n",
    "            learning_rate=0.1,\n",
    "            decay1=0.9,\n",
    "            decay2=0.999\n",
    "        )\n",
    "        return DNNApprox.create(\n",
    "            feature_functions=ff,\n",
    "            dnn_spec=self.dnn_spec,\n",
    "            adam_gradient=adam_gradient\n",
    "        )\n",
    "\n",
    "    def backward_induction_vf_and_pi(\n",
    "        self,\n",
    "        ff: Sequence[Callable[[float], float]]\n",
    "    ) -> Iterator[Tuple[DNNApprox[float], Policy[float, float]]]:\n",
    "\n",
    "        init_fa: DNNApprox[float] = self.get_vf_func_approx(ff)\n",
    "\n",
    "        mdp_f0_mu_triples: Sequence[Tuple[\n",
    "            MarkovDecisionProcess[float, float],\n",
    "            DNNApprox[float],\n",
    "            SampledDistribution[float]\n",
    "        ]] = [(\n",
    "            self.get_mdp(i),\n",
    "            init_fa,\n",
    "            self.get_states_distribution(i)\n",
    "        ) for i in range(self.time_steps())]\n",
    "\n",
    "        num_state_samples: int = 300\n",
    "        error_tolerance: float = 1e-8\n",
    "\n",
    "        return back_opt_vf_and_policy(\n",
    "            mdp_f0_mu_triples=mdp_f0_mu_triples,\n",
    "            γ=1.0,\n",
    "            num_state_samples=num_state_samples,\n",
    "            error_tolerance=error_tolerance\n",
    "        )\n",
    "\n",
    "\n",
    "if True:\n",
    "\n",
    "    from pprint import pprint\n",
    "\n",
    "    steps: int = 2\n",
    "    μ: float = 0.13\n",
    "    σ: float = 0.2\n",
    "    r: float = 0.07\n",
    "    a: float = 1.0\n",
    "    init_wealth: float = 1.0\n",
    "    init_wealth_var: float = 0.1\n",
    "\n",
    "    excess: float = μ - r\n",
    "    var: float = σ * σ\n",
    "    base_alloc: float = excess / (a * var)\n",
    "\n",
    "    risky_ret: Sequence[Gaussian] = [Gaussian(μ=μ, σ=σ) for _ in range(steps)]\n",
    "    riskless_ret: Sequence[float] = [r for _ in range(steps)]\n",
    "    utility_function: Callable[[float], float] = lambda x: - np.exp(-a * x) / a\n",
    "    alloc_choices: Sequence[float] = np.linspace(\n",
    "        2 / 3 * base_alloc,\n",
    "        4 / 3 * base_alloc,\n",
    "        11\n",
    "    )\n",
    "    feature_funcs: Sequence[Callable[[Tuple[float, float]], float]] = \\\n",
    "        [\n",
    "            lambda _: 1.,\n",
    "            lambda w_x: w_x[0],\n",
    "            lambda w_x: w_x[1],\n",
    "            lambda w_x: w_x[1] * w_x[1]\n",
    "        ]\n",
    "    dnn: DNNSpec = DNNSpec(\n",
    "        neurons=[],\n",
    "        bias=False,\n",
    "        hidden_activation=lambda x: x,\n",
    "        hidden_activation_deriv=lambda y: np.ones_like(y),\n",
    "        output_activation=lambda x: - np.sign(a) * np.exp(-x),\n",
    "        output_activation_deriv=lambda y: -y\n",
    "    )\n",
    "    init_wealth_distr: Gaussian = Gaussian(μ=init_wealth, σ=init_wealth_var)\n",
    "\n",
    "    aad: AssetAllocDiscrete = AssetAllocDiscrete(\n",
    "        risky_return_distributions=risky_ret,\n",
    "        riskless_returns=riskless_ret,\n",
    "        utility_func=utility_function,\n",
    "        risky_alloc_choices=alloc_choices,\n",
    "        feature_functions=feature_funcs,\n",
    "        dnn_spec=dnn,\n",
    "        initial_wealth_distribution=init_wealth_distr,\n",
    "        all_gaussian = True\n",
    "    )\n",
    "\n",
    "    # vf_ff: Sequence[Callable[[float], float]] = [lambda _: 1., lambda w: w]\n",
    "    # it_vf: Iterator[Tuple[DNNApprox[float], Policy[float, float]]] = \\\n",
    "    #     aad.backward_induction_vf_and_pi(vf_ff)\n",
    "\n",
    "    # print(\"Backward Induction: VF And Policy\")\n",
    "    # print(\"---------------------------------\")\n",
    "    # print()\n",
    "    # for t, (v, p) in enumerate(it_vf):\n",
    "    #     print(f\"Time {t:d}\")\n",
    "    #     print()\n",
    "    #     opt_alloc: float = p.act(init_wealth).value\n",
    "    #     val: float = v.evaluate([init_wealth])[0]\n",
    "    #     print(f\"Opt Risky Allocation = {opt_alloc:.2f}, Opt Val = {val:.3f}\")\n",
    "    #     print(\"Weights\")\n",
    "    #     for w in v.weights:\n",
    "    #         print(w.weights)\n",
    "    #     print()\n",
    "\n",
    "    it_qvf: Iterator[DNNApprox[Tuple[float, float]]] = \\\n",
    "        aad.backward_induction_qvf()\n",
    "\n",
    "    print(\"Backward Induction on Q-Value Function\")\n",
    "    print(\"--------------------------------------\")\n",
    "    print()\n",
    "    for t, q in enumerate(it_qvf):\n",
    "        print(f\"Time {t:d}\")\n",
    "        print()\n",
    "        opt_alloc: float = max(\n",
    "            ((q.evaluate([(init_wealth, ac)])[0], ac) for ac in alloc_choices),\n",
    "            key=itemgetter(0)\n",
    "        )[1]\n",
    "        val: float = max(q.evaluate([(init_wealth, ac)])[0]\n",
    "                         for ac in alloc_choices)\n",
    "        print(f\"Opt Risky Allocation = {opt_alloc:.3f}, Opt Val = {val:.3f}\")\n",
    "        print(\"Optimal Weights below:\")\n",
    "        for wts in q.weights:\n",
    "            pprint(wts.weights)\n",
    "        print()\n",
    "\n",
    "    print(\"Analytical Solution\")\n",
    "    print(\"-------------------\")\n",
    "    print()\n",
    "\n",
    "    for t in range(steps):\n",
    "        print(f\"Time {t:d}\")\n",
    "        print()\n",
    "        left: int = steps - t\n",
    "        growth: float = (1 + r) ** (left - 1)\n",
    "        alloc: float = base_alloc / growth\n",
    "        val: float = - np.exp(- excess * excess * left / (2 * var)\n",
    "                              - a * growth * (1 + r) * init_wealth) / a\n",
    "        bias_wt: float = excess * excess * (left - 1) / (2 * var) + \\\n",
    "            np.log(np.abs(a))\n",
    "        w_t_wt: float = a * growth * (1 + r)\n",
    "        x_t_wt: float = a * excess * growth\n",
    "        x_t2_wt: float = - var * (a * growth) ** 2 / 2\n",
    "\n",
    "        print(f\"Opt Risky Allocation = {alloc:.3f}, Opt Val = {val:.3f}\")\n",
    "        print(f\"Bias Weight = {bias_wt:.3f}\")\n",
    "        print(f\"W_t Weight = {w_t_wt:.3f}\")\n",
    "        print(f\"x_t Weight = {x_t_wt:.3f}\")\n",
    "        print(f\"x_t^2 Weight = {x_t2_wt:.3f}\")\n",
    "        print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2) SARSA\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
