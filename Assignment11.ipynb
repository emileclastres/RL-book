{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 11 : RL Prediction (part 1)\n",
    "\n",
    "## 1) Tabular MC from scratch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Here are the contents of rl/monte\\_carlo.py :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Monte Carlo methods for working with Markov Reward Process and\n",
    "Markov Decision Processes.\n",
    "\n",
    "'''\n",
    "\n",
    "from typing import Iterable, Iterator, Tuple, TypeVar, Callable, Mapping\n",
    "\n",
    "from rl.distribution import Distribution\n",
    "from rl.function_approx import FunctionApprox\n",
    "import rl.markov_process as mp\n",
    "import rl.markov_decision_process as markov_decision_process\n",
    "from rl.markov_decision_process import (MarkovDecisionProcess)\n",
    "from rl.returns import returns\n",
    "\n",
    "S = TypeVar('S')\n",
    "A = TypeVar('A')\n",
    "\n",
    "\n",
    "def mc_prediction(\n",
    "        traces: Iterable[Iterable[mp.TransitionStep[S]]],\n",
    "        approx_0: FunctionApprox[S],\n",
    "        γ: float,\n",
    "        tolerance: float = 1e-6\n",
    ") -> Iterator[FunctionApprox[S]]:\n",
    "    '''Evaluate an MRP using the monte carlo method, simulating episodes\n",
    "    of the given number of steps.\n",
    "\n",
    "    Each value this function yields represents the approximated value\n",
    "    function for the MRP after one additional epsiode.\n",
    "\n",
    "    Arguments:\n",
    "      traces -- an iterator of simulation traces from an MRP\n",
    "      approx_0 -- initial approximation of value function\n",
    "      γ -- discount rate (0 < γ ≤ 1), default: 1\n",
    "      tolerance -- a small value—we stop iterating once γᵏ ≤ tolerance\n",
    "\n",
    "    Returns an iterator with updates to the approximated value\n",
    "    function after each episode.\n",
    "\n",
    "    '''\n",
    "    episodes = (returns(trace, γ, tolerance) for trace in traces)\n",
    "\n",
    "    return approx_0.iterate_updates(\n",
    "        ((step.state, step.return_) for step in episode)\n",
    "        for episode in episodes\n",
    "    )\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to make it pretty general, allowing the user to specify a function counts\\_to\\_weights that we will call $f$ to stick with the lecture's notation.\n",
    "\n",
    "The inputs are traces, $\\gamma$ and tolerance (unchanged). We replace approx_0 by the function $f$ used for tabular updates. The function outputs a Mapping from states (assumed finite) to floats representing the final value function $V$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mc_tabular_prediction(\n",
    "        traces: Iterable[Iterable[mp.TransitionStep[S]]],\n",
    "        γ: float,\n",
    "        f : Callable[[int], float] = lambda n : 1/n,\n",
    "        tolerance: float = 1e-6\n",
    ") -> Mapping[S, float]:\n",
    "    \n",
    "    table  :  Mapping[S, float] = dict()\n",
    "    counts :  Mapping[S, int]   = dict()\n",
    "    \n",
    "    episodes = (returns(trace, γ, tolerance) for trace in traces)\n",
    "    \n",
    "    for episode in episodes:\n",
    "        for step in episode:\n",
    "            counts[step.state] = counts.get(step.state, 0) + 1\n",
    "            if counts[step.state] == 1:\n",
    "                table[step.state] = step.return_\n",
    "            else:\n",
    "                table[step.state] = step.return_ * f(counts[step.state])+(1-f(counts[step.state]))*table.get(step.state, 0.)\n",
    "    return table\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) Tabular Temporal-Difference from scratch\n",
    "\n",
    "#### Here are the contents of rl/td.py : \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Callable, Iterable, Iterator, TypeVar, Tuple\n",
    "\n",
    "from rl.function_approx import FunctionApprox\n",
    "import rl.markov_process as mp\n",
    "import rl.markov_decision_process as mdp\n",
    "import rl.iterate as iterate\n",
    "\n",
    "S = TypeVar('S')\n",
    "\n",
    "\n",
    "def td_prediction(\n",
    "        transitions: Iterable[mp.TransitionStep[S]],\n",
    "        approx_0: FunctionApprox[S],\n",
    "        γ: float,\n",
    ") -> Iterator[FunctionApprox[S]]:\n",
    "    '''Evaluate an MRP using TD(0) using the given sequence of\n",
    "    transitions.\n",
    "\n",
    "    Each value this function yields represents the approximated value\n",
    "    function for the MRP after an additional transition.\n",
    "\n",
    "    Arguments:\n",
    "      transitions -- a sequence of transitions from an MRP which don't\n",
    "                     have to be in order or from the same simulation\n",
    "      approx_0 -- initial approximation of value function\n",
    "      γ -- discount rate (0 < γ ≤ 1)\n",
    "\n",
    "    '''\n",
    "    def step(v, transition):\n",
    "        return v.update([(transition.state,\n",
    "                          transition.reward + γ * v(transition.next_state))])\n",
    "\n",
    "    return iterate.accumulate(transitions, step, initial=approx_0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## In a similar fashion, we replace approx\\_0 by the specification of f : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def td_tabular_prediction(\n",
    "        transitions: Iterable[mp.TransitionStep[S]],\n",
    "        γ: float,\n",
    "        f : Callable[[int], float] = lambda n : 1/n\n",
    ") -> Mapping[S, float]:\n",
    "\n",
    "    \n",
    "    table  :  Mapping[S, float] = dict()\n",
    "    counts :  Mapping[S, int]   = dict()\n",
    "\n",
    "    for step in transitions:\n",
    "        counts[step.state] = counts.get(step.state, 0) + 1\n",
    "        if counts[step.state] == 1:\n",
    "            table[step.state] = step.reward\n",
    "        else:\n",
    "            table[step.state] = (step.reward + γ * table.get(step.next_state, 0.) - table.get(step.state, 0.)) * f(counts[step.state])+ table.get(step.state, 0.)\n",
    "    return table\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3) Let's test and compare our functions on the Simple Inventory example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Value Function\n",
      "--------------\n",
      "{InventoryState(on_hand=1, on_order=0): -28.932,\n",
      " InventoryState(on_hand=1, on_order=1): -29.345,\n",
      " InventoryState(on_hand=2, on_order=0): -30.345,\n",
      " InventoryState(on_hand=0, on_order=0): -35.511,\n",
      " InventoryState(on_hand=0, on_order=1): -27.932,\n",
      " InventoryState(on_hand=0, on_order=2): -28.345}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from rl.chapter2.simple_inventory_mrp import SimpleInventoryMRPFinite, InventoryState\n",
    "\n",
    "user_capacity = 2\n",
    "user_poisson_lambda = 1.0\n",
    "user_holding_cost = 1.0\n",
    "user_stockout_cost = 10.0\n",
    "\n",
    "user_gamma = 0.9\n",
    "\n",
    "si_mrp = SimpleInventoryMRPFinite(\n",
    "    capacity=user_capacity,\n",
    "    poisson_lambda=user_poisson_lambda,\n",
    "    holding_cost=user_holding_cost,\n",
    "    stockout_cost=user_stockout_cost\n",
    ")\n",
    "\n",
    "print(\"Value Function\")\n",
    "print(\"--------------\")\n",
    "si_mrp.display_value_function(gamma=user_gamma)\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MC converges fast with simple averaging as $f$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{InventoryState(on_hand=0, on_order=0): -35.37499259296973,\n",
       " InventoryState(on_hand=0, on_order=2): -28.188519503557927,\n",
       " InventoryState(on_hand=1, on_order=0): -28.885416754229663,\n",
       " InventoryState(on_hand=0, on_order=1): -27.832750921069934,\n",
       " InventoryState(on_hand=1, on_order=1): -29.261087321824473,\n",
       " InventoryState(on_hand=2, on_order=0): -30.22427418120901}"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from itertools import islice\n",
    "episodes = islice(si_mrp.reward_traces(Constant(InventoryState(0,0))), 100)\n",
    "table_mc = mc_tabular_prediction(episodes, user_gamma)\n",
    "table_mc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is not the same order of keys, but the MC value function is the same as that obtained analytically.\n",
    "Let's compute it with TD :"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TD with naive average as f is not converging ...\n",
    "\n",
    "### This is to be expected since the initial boostrapped estimates are very bad and we must forget what was learned at the beginnning of training to get rid of the bias...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{InventoryState(on_hand=0, on_order=0): -17.589905522960837,\n",
       " InventoryState(on_hand=0, on_order=2): -10.602750178641903,\n",
       " InventoryState(on_hand=2, on_order=0): -12.916651578910775,\n",
       " InventoryState(on_hand=1, on_order=0): -12.338258635044827,\n",
       " InventoryState(on_hand=1, on_order=1): -12.117442034710983,\n",
       " InventoryState(on_hand=0, on_order=1): -11.4163098742954}"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transitions = islice(si_mrp.simulate_reward(Constant(InventoryState(0,0))), 1000)\n",
    "table_td = td_tabular_prediction(transitions, user_gamma)\n",
    "table_td"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{InventoryState(on_hand=0, on_order=0): -24.72484422537357,\n",
       " InventoryState(on_hand=0, on_order=2): -17.57673908729679,\n",
       " InventoryState(on_hand=2, on_order=0): -19.54157923812064,\n",
       " InventoryState(on_hand=1, on_order=0): -18.179451123618797,\n",
       " InventoryState(on_hand=0, on_order=1): -17.185760282974428,\n",
       " InventoryState(on_hand=1, on_order=1): -18.528461587007595}"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transitions = islice(si_mrp.simulate_reward(Constant(InventoryState(0,0))), 100000)\n",
    "table_td = td_tabular_prediction(transitions, user_gamma)\n",
    "table_td"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Switching to an exponential decay as learning rate grants us convergence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{InventoryState(on_hand=0, on_order=0): -35.839584609262296,\n",
       " InventoryState(on_hand=0, on_order=2): -28.551217393378355,\n",
       " InventoryState(on_hand=1, on_order=0): -29.144318746166075,\n",
       " InventoryState(on_hand=0, on_order=1): -28.296515448516118,\n",
       " InventoryState(on_hand=1, on_order=1): -29.46196739301054,\n",
       " InventoryState(on_hand=2, on_order=0): -30.1767184009853}"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transitions = islice(si_mrp.simulate_reward(Constant(InventoryState(0,0))), 10000)\n",
    "table_td = td_tabular_prediction(transitions, user_gamma, f = lambda n : 0.05)\n",
    "table_td"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RandomWalk2D : strolling on a grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
