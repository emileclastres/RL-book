{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stanford CME 241 (Winter 2021) - Assignment 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let $\\pi_D$ be a deterministic policy. The 4 Bellman equations become:\n",
    "\n",
    "* $$V^{\\pi_D}(s) = Q^{\\pi_D}(s, \\pi_D(s))$$\n",
    "* $$Q^{\\pi_D}(s,a) = \\mathcal{R}(s,a) + \\gamma \\cdot \\sum_{s' \\in \\mathcal{N}}\\mathcal{P}(s,a,s') \\cdot Q^{\\pi_D}$$\n",
    "* $$V^{\\pi_D}(s) = \\mathcal{R}(s,a) + \\gamma \\cdot \\sum_{s' \\in \\mathcal{N}}\\mathcal{P}(s, \\pi_D (s), s') \\cdot V^{\\pi_D}(s') $$\n",
    "* $$Q^{\\pi_D}(s,a) = \\mathcal{R}(s,a) + \\gamma \\cdot \\sum_{s' \\in \\mathcal{N}}\\mathcal{P}(s,a,s')Q^{\\pi_D}(s', \\pi_D (s')) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The definition of the given Markov process is independant of the state s. Therefore, we get $V^*(s) = V^*(s') ~~ \\forall s,s'\\in \\mathcal{S}$.\n",
    "Let us derive $V^*$ using this identity: \n",
    "\n",
    "$$V^*(s) = max_a [ \\mathcal{R}(s,a) + \\frac{1}{2}((1-a)+a)\\cdot V^*(s)]$$\n",
    "$$V^*(s) = max_a [ 2a(1-a)] + \\frac{1}{2}\\cdot V^*(s)$$\n",
    "### Which yields $V^*(s) = 1$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly, we compute $$Q^*(s,a) = 2a(1-a) + \\frac{1}{2}$$\n",
    "We the get an optimal policy $$\\pi_D^* (s) = argmax_a [Q^*(s,a)] = \\frac{1}{2}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. A patient frog vs. a busy frog ðŸ¸\n",
    " #### Bonus assignment : what would the frog do if she was in a rush to escape the pond ? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "State spaces : $\\mathcal{S} = \\{0...n\\}$, $\\mathcal{N} = \\{1...(n-1)\\}$, $\\mathcal{T} = \\{0,n\\}$\n",
    "Action space:  $\\mathcal{A} = \\{A,B\\}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we are only interested in the survival of the frog, the rewards are sparse and $\\gamma$ = 1. We will therefore define the MDP with $\\mathcal{P}$ and $\\mathcal{R_T}$:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*  $\\mathcal{R_T}(s,a, s') =  1 \\cdot \\{s'=n\\} $\n",
    "* $\\mathcal{P}(s,A,s-1) = 1 - \\mathcal{P}(s,A,s+1) = \\frac{s}{n}$\n",
    "* $\\mathcal{P}(s,B,s') = \\frac{1}{n} ~~ \\forall s' \\neq s$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rl.markov_decision_process import FiniteMarkovDecisionProcess, FinitePolicy\n",
    "from rl.distribution import Categorical, Constant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Frog(FiniteMarkovDecisionProcess):\n",
    "    def __init__(self, n):\n",
    "        def reward(i):\n",
    "            return float(i==n)-float(i==0) \n",
    "        mapping = {}\n",
    "        mapping[0] = None\n",
    "        mapping[n] = None\n",
    "        for i in range(1,n):\n",
    "            mapping[i]={}\n",
    "            mapping[i]['A'] = Categorical({(i-1, reward(i-1)):i/n, (i+1, reward(i+1)):(n-i)/n })\n",
    "            mapping[i]['B'] = Categorical({(j, reward(j)):1/n for j in range(0,n+1) if j!=i})\n",
    "        super().__init__(mapping)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "def enumerate_all_deterministic_policies(k):\n",
    "    def enumerate_all_combinations(n):\n",
    "        assert n >=0\n",
    "        if n == 0:\n",
    "            return [{0:None}]\n",
    "        elif n==1:\n",
    "            return [{0: None, 1:None}]\n",
    "        else:\n",
    "            res =[]\n",
    "            policies_previous = enumerate_all_combinations(n-1)\n",
    "            for policy in policies_previous:\n",
    "                policy[n] = None\n",
    "                p1, p2 = policy.copy(), policy.copy()\n",
    "                p1[n-1] = Constant('A')\n",
    "                p2[n-1] = Constant('B')\n",
    "                res.append(p1)\n",
    "                res.append(p2)\n",
    "            return res\n",
    "    policies = enumerate_all_combinations(k)\n",
    "    return [FinitePolicy(policy) for policy in policies]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def brute_force_optimal_V(n, gamma = 1.):\n",
    "    policies = enumerate_all_deterministic_policies(n)\n",
    "\n",
    "    for i,policy in enumerate(policies):\n",
    "        V = Frog(n).apply_finite_policy(policy).get_value_function_vec(gamma)\n",
    "        if i == 0:\n",
    "            V_star = V\n",
    "            Pi_star = policy\n",
    "        elif np.all(V >= V_star):\n",
    "            V_star = V\n",
    "            Pi_star = policy\n",
    "    return V_star, Pi_star"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 is a Terminal State\n",
      "For State 1:\n",
      "  Do Action B with Probability 1.000\n",
      "For State 2:\n",
      "  Do Action A with Probability 1.000\n",
      "For State 3:\n",
      "  Do Action A with Probability 1.000\n",
      "For State 4:\n",
      "  Do Action A with Probability 1.000\n",
      "For State 5:\n",
      "  Do Action A with Probability 1.000\n",
      "For State 6:\n",
      "  Do Action A with Probability 1.000\n",
      "For State 7:\n",
      "  Do Action A with Probability 1.000\n",
      "For State 8:\n",
      "  Do Action A with Probability 1.000\n",
      "For State 9:\n",
      "  Do Action A with Probability 1.000\n",
      "For State 10:\n",
      "  Do Action A with Probability 1.000\n",
      "For State 11:\n",
      "  Do Action A with Probability 1.000\n",
      "12 is a Terminal State\n",
      "\n"
     ]
    }
   ],
   "source": [
    "V_star, Pi_star  = brute_force_optimal_V(12)\n",
    "print(Pi_star)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.34789534 0.39567691 0.40523322 0.40841866 0.41001138 0.41114903\n",
      " 0.41228669 0.41387941 0.41706485 0.42662116 0.47440273]\n"
     ]
    }
   ],
   "source": [
    "print(V_star)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The optimal strategy is to always croak A, except when in state 1. This makes sense: In state all states but state 1 it is riskless to go one step backward. Let's see what it becomes if we give an incentive to escape quickly from the pond : let's set $\\gamma = 0.99$ for a patient frog, and then  $\\gamma = 0.01$ for a frog with no time to waste."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 is a Terminal State\n",
      "For State 1:\n",
      "  Do Action B with Probability 1.000\n",
      "For State 2:\n",
      "  Do Action B with Probability 1.000\n",
      "For State 3:\n",
      "  Do Action B with Probability 1.000\n",
      "For State 4:\n",
      "  Do Action B with Probability 1.000\n",
      "For State 5:\n",
      "  Do Action B with Probability 1.000\n",
      "For State 6:\n",
      "  Do Action B with Probability 1.000\n",
      "For State 7:\n",
      "  Do Action B with Probability 1.000\n",
      "For State 8:\n",
      "  Do Action A with Probability 1.000\n",
      "For State 9:\n",
      "  Do Action A with Probability 1.000\n",
      "For State 10:\n",
      "  Do Action A with Probability 1.000\n",
      "For State 11:\n",
      "  Do Action A with Probability 1.000\n",
      "12 is a Terminal State\n",
      "\n"
     ]
    }
   ],
   "source": [
    "V_star, Pi_star  = brute_force_optimal_V(12, gamma = 0.99)\n",
    "print(Pi_star)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 is a Terminal State\n",
      "For State 1:\n",
      "  Do Action B with Probability 1.000\n",
      "For State 2:\n",
      "  Do Action B with Probability 1.000\n",
      "For State 3:\n",
      "  Do Action B with Probability 1.000\n",
      "For State 4:\n",
      "  Do Action B with Probability 1.000\n",
      "For State 5:\n",
      "  Do Action B with Probability 1.000\n",
      "For State 6:\n",
      "  Do Action B with Probability 1.000\n",
      "For State 7:\n",
      "  Do Action B with Probability 1.000\n",
      "For State 8:\n",
      "  Do Action B with Probability 1.000\n",
      "For State 9:\n",
      "  Do Action B with Probability 1.000\n",
      "For State 10:\n",
      "  Do Action A with Probability 1.000\n",
      "For State 11:\n",
      "  Do Action A with Probability 1.000\n",
      "12 is a Terminal State\n",
      "\n"
     ]
    }
   ],
   "source": [
    "V_star, Pi_star  = brute_force_optimal_V(12, gamma = 0.1)\n",
    "print(Pi_star)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### As expected, the frog is now willing to croak B to move faster (which is riskier than doing the conservative $\\gamma =1$ strategy). The smaller $\\gamma$, the more our frog will croak B (in positions 1,2,...,k) to try to teleport quickly towards the $n^{th}$ position."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$Q^* (s,a) = \\mathcal{R}(s,a) = \\frac{1}{\\sqrt{2\\pi}\\sigma}\\cdot \\int_{s'\\in R}~e^{as'}\\cdot e^{\\frac{(s'-s)^2}{2 \\sigma^2}}ds'  $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
